
#This is just placeholder
import tensorflow as tf
import numpy as np
import tensorflow_datasets as tfds
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt
train_ds, test_ds = tfds.load('emnist/letters', 
                             split=['train', 'test'],
                             as_supervised=True)
X = []
Y = []
for img, lbl in tfds.as_numpy(train_ds):
    img = img.reshape(-1)
    img = img / 255.0
    X.append(img)
    Y.append(lbl)
2025-04-28 20:03:19.974687: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608
2025-04-28 20:03:23.491580: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
X_train = np.array(X)
Y_train = np.array(Y)
Y_train = Y_train - 1
#Shuffle the arrays with the same seed so the relative order is the same.
np.random.seed(1234)
perm = np.random.permutation(X_train.shape[0])
X_train = X_train[perm]
Y_train = Y_train[perm]
model = Sequential([
    tf.keras.Input(shape=(784,)),
    Dense(512, activation = 'relu', name = 'layer_1', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    Dense(256, activation = 'relu', name = 'layer_2', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    Dense(128, activation = 'relu', name = 'layer_3', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    Dense(26, activation = 'linear', name = 'output')
])
model.summary()
Model: "sequential"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ layer_1 (Dense)                 │ (None, 512)            │       401,920 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ layer_2 (Dense)                 │ (None, 256)            │       131,328 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ layer_3 (Dense)                 │ (None, 128)            │        32,896 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ output (Dense)                  │ (None, 26)             │         3,354 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 569,498 (2.17 MB)
 Trainable params: 569,498 (2.17 MB)
 Non-trainable params: 0 (0.00 B)
model.compile(
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005),
    metrics = ['accuracy']
)
history = model.fit (
    X_train, Y_train,
    epochs = 30,
    batch_size = 36,
    shuffle=True
)
Epoch 1/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.6867 - loss: 1.7876
Epoch 2/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.8642 - loss: 0.8215
Epoch 3/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.8807 - loss: 0.6786
Epoch 4/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.8908 - loss: 0.6032
Epoch 5/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.8953 - loss: 0.5658
Epoch 6/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.8997 - loss: 0.5370
Epoch 7/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.9052 - loss: 0.5123
Epoch 8/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.9072 - loss: 0.4949
Epoch 9/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.9077 - loss: 0.4833
Epoch 10/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.9118 - loss: 0.4755
Epoch 11/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.9127 - loss: 0.4619
Epoch 12/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9121 - loss: 0.4558
Epoch 13/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9137 - loss: 0.4476
Epoch 14/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9146 - loss: 0.4389
Epoch 15/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9155 - loss: 0.4343
Epoch 16/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9180 - loss: 0.4261
Epoch 17/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9181 - loss: 0.4245
Epoch 18/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9186 - loss: 0.4199
Epoch 19/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.9175 - loss: 0.4171
Epoch 20/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.9204 - loss: 0.4076
Epoch 21/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9204 - loss: 0.4050
Epoch 22/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.9198 - loss: 0.4035
Epoch 23/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9212 - loss: 0.4027
Epoch 24/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.9222 - loss: 0.3996
Epoch 25/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9223 - loss: 0.3958
Epoch 26/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9219 - loss: 0.3926
Epoch 27/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9227 - loss: 0.3874
Epoch 28/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9232 - loss: 0.3914
Epoch 29/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.9231 - loss: 0.3856
Epoch 30/30
2467/2467 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9253 - loss: 0.3814
plt.plot(history.history['loss'], label='Training Loss')
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss Curve')
plt.show()
No description has been provided for this image
X = [] 
Y = []
for img, lbl in tfds.as_numpy(test_ds):
    img = img.reshape(-1)
    img = img / 255.0
    X.append(img)
    Y.append(lbl)
X_test = np.array(X)
Y_test = np.array(Y)
single_example = X_test[2:3]
prediction = model.predict(single_example)
distribution = tf.nn.softmax(prediction).numpy()

predicted_label = np.argmax(distribution)

print(chr(predicted_label + ord('A')))  # Predicted letter
print(chr(Y_test[2] + ord('A')))         # True letter
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step
A
B
test_loss, test_accuracy = model.evaluate(X_test, Y_test)
print("Test Accuracy", test_accuracy)
463/463 ━━━━━━━━━━━━━━━━━━━━ 0s 576us/step - accuracy: 0.0053 - loss: 12.0173
Test Accuracy 0.004527027253061533

